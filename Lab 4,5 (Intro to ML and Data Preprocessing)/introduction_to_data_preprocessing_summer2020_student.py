# -*- coding: utf-8 -*-
"""Introduction to Data Preprocessing_Summer2020_Student

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_794R2HWZJYT06Q2iXBouXsrzXX-bQ_P

## CSE 422 Introduction to Data Preprocessing
---

### What are the advantages of preprocessing the data before applying on machine learning algorithm?

"The biggest advantage of pre-processing in ML is to improve **generalizablity** of your model. Data for any ML application is collected through some ‘sensors’. These sensors can be physical devices, instruments, software programs such as web crawlers, manual surveys, etc. Due to hardware malfunctions, software glitches, instrument failures, amd human errors, noise and erroneous information may creep in that can severely affect the performance of your model. Apart from **noise**, there are several **redundant information** that needs to be removed. For e.g. while predicting whether it rains tomorrow or not, age of the person is irrelevant. In terms of text processing, there are several stop words that may be redundant for the analysis. Lastly, there may be several **outliers** present in your data, due to the way data is collected that may need to be removed to improve the performance of the classifiers." 
                                    
                                            -Shehroz Khan, ML Researcher, Postdoc @U of Toronto

Some Data Preprocessing Techniques:

* Deleting duplicate and null values
* Imputation for missing values
* Handling Categorical Features
* Feature Normalization/Scaling
* Feature Engineering
* Feature Selection
"""

#importing necessary libraries
import pandas as pd
import numpy as np

"""#Removing Null values / Handling Missing data



"""

volunteer = pd.read_csv('/content/sample_data/volunteer_opportunities.csv')
volunteer.head(3)

volunteer.shape

volunteer.isnull().sum()

"""dropping columns"""

volunteer = volunteer.drop(['BIN', 'BBL', 'NTA'], axis = 1)
volunteer.shape

"""dropping rows"""

# Check how many values are missing in the category_desc column
print("Number of rows with null values in category_desc column: ", volunteer['category_desc'].isnull().sum())

# Subset the volunteer dataset

volunteer_subset = volunteer[volunteer['category_desc'].notnull()]

# Print out the shape of the subset
print("Shape after removing null values: ", volunteer_subset.shape)

print("Shape of dataframe before dropping:", volunteer.shape)
volunteer = volunteer.dropna(axis = 0, subset = ['category_desc'])
print("Shape after dropping:", volunteer.shape)

"""### Imputing missing Values"""

sales = pd.read_csv('/content/sample_data/sales.csv', index_col = ['month'])
sales

sales.fillna(50)

sales = pd.read_csv('/content/sample_data/sales.csv', index_col = ['month'])

sales[['salt']]

from sklearn.impute import SimpleImputer

impute = SimpleImputer(missing_values=np.nan, strategy='mean')

impute.fit(sales[['salt']])

sales['salt'] = impute.transform(sales[['salt']])

sales

"""## Standardizing Data

## Feature Scaling

## Why do we need to scale our data?
* If a feature’s variance is orders of magnitude more than the variance of other features, that particular feature might dominate other features in the dataset and make the estimator unable to learn from other features correctly, i.e. our learner might give more importance to features with high variance, which is not something we want happening in our model.

The following are a few different types of Scalers:

**MinMax Scaler:** 

Scales values to a range between 0 and 1 if no negative values, and -1 to 1 if there are negative values present.

$$\frac{X - X_{min}}{X_{max} - X_{min}}$$

where, 

 $$X\space is\space a\space feature\space value.$$ 
 $$X_{min} \space and \space X_{max} \space are \space corresponding \space feature's \space min \space and \space max \space values. $$


**Standard Scaler:**

$$\frac{X - mean}{\sigma}$$
where,
$$\sigma = standard \space deviation $$ 

**Robust Scaler:**

Uses statistics that are robust to outliers

$$\frac{X - median}{IQR}$$

where, 

$$ IQR = Inter\space Quartile\space Range = Q_3 - Q_1 $$

Sklearn library provides functions for different scalers by which we can easily scale our data.
"""

from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split

cancer = load_breast_cancer()

X_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target,
                                                    random_state=1)
print(X_train.shape)
print(X_test.shape)

from sklearn.preprocessing import MinMaxScaler

scaler = MinMaxScaler()

scaler.fit(X_train)

# transform data
X_train_scaled = scaler.transform(X_train)

"""We can see that after Min-Max Scaling all the values are in the range [0,1]"""

print("per-feature minimum before scaling:\n {}".format(X_train.min(axis=0)))
print("per-feature maximum before scaling:\n {}".format(X_train.max(axis=0)))

print("per-feature minimum after scaling:\n {}".format(
    X_train_scaled.min(axis=0)))
print("per-feature maximum after scaling:\n {}".format(
    X_train_scaled.max(axis=0)))

# transform test data
X_test_scaled = scaler.transform(X_test)

"""## Effect of using MinMax Scaler:

### Accuracy without scaling
"""

from sklearn.neighbors import KNeighborsClassifier
X_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target,
                                                    random_state=0)
knn=KNeighborsClassifier()

knn.fit(X_train, y_train)

print("Test set accuracy: {:.2f}".format(knn.score(X_test, y_test)))

"""### We can see that accuracy improves if we train on scaled data."""

# preprocessing using 0-1 scaling
scaler = MinMaxScaler()
scaler.fit(X_train)

X_train_scaled = scaler.transform(X_train)

X_test_scaled = scaler.transform(X_test)

#train
knn.fit(X_train_scaled, y_train)

# scoring on the scaled test set
print("Scaled test set accuracy: {:.2f}".format(
    knn.score(X_test_scaled, y_test)))

"""### Effect using Standard Scaler: 
We can see that accuracy has improved compared to the non-scaled version, but we can infer that for this problem, Standard Scaler performs worse than MinMaxScaler.
"""

from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()

#instead of using .fit() and .transform() separately, we can use .fit_transform()
X_scaled_d = scaler.fit_transform(X_train)

# preprocessing using zero mean and unit variance scaling
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
scaler.fit(X_train)

X_train_scaled = scaler.transform(X_train)
X_test_scaled = scaler.transform(X_test)

# learning an SVM on the scaled training data
knn.fit(X_train_scaled, y_train)

# scoring on the scaled test set
print("KNN test accuracy: {:.2f}".format(knn.score(X_test_scaled, y_test)))

"""## Feature Engineering

### Encoding categorical variables - binary
"""

hiking = pd.read_json('/content/sample_data/hiking.json')
hiking.head()

hiking.info()

hiking['Accessible'].unique()

from sklearn.preprocessing import LabelEncoder

# Set up the LabelEncoder object
enc = LabelEncoder()

# Apply the encoding to the "Accessible" column
hiking['Accessible_enc'] = enc.fit_transform(hiking['Accessible'])

# Compare the two columns
print(hiking[['Accessible', 'Accessible_enc']].head())

"""We may also encode/map a certain class to a specific code (e.g 0/1/2) by using the `map()` function. """

hiking['Accessible'] = hiking['Accessible'].map({'good':2,'bad':0,'average':1})

volunteer = pd.read_csv('/content/sample_data/volunteer_opportunities.csv')
volunteer.head(3)

"""### Encoding categorical variables - one-hot encoding"""

volunteer.info()

volunteer['category_desc'].unique()

# Transform the category_desc column
category_enc = pd.get_dummies(volunteer['category_desc'])

# Take a look at the encoded columns
category_enc.head()

volunteer['category_desc'].head()

"""##Feature Engineering

### Engineering numerical features - by taking an average

Suppose we have multiple features each of which contains time taken for each runner to complete a lap. We can reduce the dimensionality of our dataset (reduce the number of features) by averaging(mean) the time taken of each run.
"""

running_times_5k = pd.DataFrame([['Sue', 20.1, 18.5, 19.6, 20.3, 18.3], ['Mark', 16.5, 17.1, 16.9, 17.6, 17.3], ['Sean', 23.5, 25.1, 25.2, 24.6, 23.9], ['Erin', 21.7, 21.1, 20.9, 22.1, 22.2], ['Jenny', 25.8, 27.1, 26.1, 26.7, 26.9], ['Russell', 30.9, 29.6, 31.4, 30.4, 29.9]])

running_times_5k.columns =  ['name', 'run1', 'run2', 'run3', 'run4', 'run5']

running_times_5k

# Create a list of the columns to average
run_columns = ["run1", "run2", "run3", "run4", "run5"]

# Use apply to create a mean column
running_times_5k["mean"] = running_times_5k.apply(lambda row: row[run_columns].mean(), axis=1)

# Take a look at the results
print(running_times_5k)

"""### Engineering numerical features - datetime

We are extracting the start_date_month from the `date_time` format which we can use later to input into our learner/model/algorithm. This feature will be much more meaningful for the learner compared to the `date_time` format. 
"""

volunteer["start_date_date"]

# First, convert string column to date column
volunteer["start_date_converted"] = pd.to_datetime(volunteer["start_date_date"])

# Extract just the month from the converted column
volunteer["start_date_month"] = volunteer["start_date_converted"].apply(lambda row: row.month)

# Take a look at the converted and new month columns
volunteer[['start_date_converted', 'start_date_month']].head()

"""## Feature Selection

### Selecting relevant features

Sometimes certain circumstances arise that we have a lot of features in our dataset, but from our prior/domain knowledge we know that certain features might not be too important. In such cases we may want to drop those irrelevant features.
"""

# Create a list of redundant column names to drop
to_drop = ["category_desc", "created_date", "locality", "region", "vol_requests"]

# Drop those columns from the dataset
volunteer_subset = volunteer.drop(to_drop, axis=1)

# Print out the head of the new dataset
volunteer_subset.head()

"""### Checking for correlated features

We may use the following heatmap to find out the correlation between each of the features in a dataset. If a certain feature is highly correlated with more than one feature, we may choose to drop that feature (in this case it is *flavanoids*) because it will affect our model in a similar way as the other two features (and thus will prove to redundant). Correlation between two features may be found using the color gradient shown on the right.
"""

from sklearn.datasets import load_wine

wine = load_wine()
wine_df = pd.DataFrame( wine['data'], columns=wine['feature_names'])

wine_df.head()

wine_corr = wine_df.corr()
wine_corr

import seaborn as sns

sns.heatmap(wine_corr, cmap = 'YlGnBu')

# Take a minute to find the column where the correlation value is greater than 0.75 at least twice
to_drop = 'flavanoids'

# Drop that column from the DataFrame
wine_df = wine_df.drop(to_drop, axis=1)

"""## **Summary:**

Basic Pipeline for solving a ML project:

1. Read in Dataset

2. Get to know your dataset using data vizualisation and other techniques

3. Preprocess your dataset:

  * remove/impute null values
  * remove outliers
  * feature scaling
  * feature engineering
  * feature selection

4. train/test split
5. choose and build (number of) machine learning algorithm
5. train model on training data
6. make prediction on test data
7. evaluate performance on test data
8. visualization of your results

---
.

.

**Reference**


* Müller Andreas Christian, and Sarah Guido. Introduction to Machine Learning with Python a Guide for Data Scientists. OReilly, 2018.

* DataCamp Python Course
"""